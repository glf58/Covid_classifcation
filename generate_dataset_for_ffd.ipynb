{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_dataset_for_ffd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPARFNgt3yu6VvxpGs5bCwC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/glf58/Projet_Covid/blob/GLF/generate_dataset_for_ffd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectif\n",
        "Ce notebook sert a generer un dataset dans un format compatible avec la methode flow_from_directory. Voir la definition de la fonction generate_and_split_data.\n",
        "On commence par lire les donnees dans l'archive, puis, une fois les images distribuees, on les redistribue dans les bons repertoires. On peut egalement creer et sauver une archive (.zip) de ces donnees qu'il suffira d'aller lire pour entrainer le modele."
      ],
      "metadata": {
        "id": "6gN6RBYyWxzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fPxi9VJwWwl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wQk1QxVMyij"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import time\n",
        "import shutil\n",
        "import datetime \n",
        "\n",
        "from skimage import exposure\n",
        "\n",
        "from google.colab import drive\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from tensorflow.keras.utils import load_img, img_to_array, save_img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj12_iN2M5-O",
        "outputId": "9b0eae54-60b6-49ed-860a-e636c13e5e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "archive='/content/drive/MyDrive/kaggle/covid19-radiography-database.zip'\n",
        "with ZipFile(archive, 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content')"
      ],
      "metadata": {
        "id": "7unPYDB4PVtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_data='/content/COVID-19_Radiography_Dataset' #repertoire ou on dezippe les images initiales\n",
        "path_out = '/content/C19' #repertoire ou on va copier les images qu'on voudra zipper\n",
        "save_archive = True       #True pour sauver l'archive dans le drive perso, False sinon; dans ce cas les donnees seront perdues apres la session de colab\n",
        "histo_equalize = True     #True pour renormaliser les expositions, False pour garder les images d'origine\n",
        "if histo_equalize:\n",
        "  he='histo_equal'\n",
        "else:\n",
        "  he=''\n",
        "filename = '/content/drive/MyDrive/kaggle/covid19-'+he+'-dataset_for_ffd'  #chemin de destination de l'archive que je sauve dans mon drive personnel afin de ne pas le perdre.\n",
        "data_types=['Test', 'Validation', 'Train']\n",
        "cats = ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia']\n",
        "initial_size = (299,299)  #taille initiale des images\n",
        "target_size = (299,299)   #redimensionnement des images\n",
        "Nmax = 11200              #nb max d'images par categorie a considerer\n",
        "test_size = 15            #pourcentage images dans le repertoire de test: 15 pour 15%\n",
        "val_size = 15             #pourcentage images dans le repertoire de validation: 15 pour 15%"
      ],
      "metadata": {
        "id": "aKl3BuMhM9iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_split_data(test_size, val_size, cats, data_types, Nmax, histo_equalize, path_in, path_out, save_archive, filename):\n",
        "  ### cette fonction va lire les images dans 'path_in' et creer une arborescence compatible avec la methode flow_from_directory dans le repertoire 'path_out'.\n",
        "  ### Chaque image sera lue dans le repertoire 'path_in' et collee dans le bon repertoire cible de 'path_out'. On considere que les premieres images iront \n",
        "  ### dans le repertoire de 'Test' et les autres dans le repertoire 'Train'. Pour chaque categorie, la somme du nombre d'images transferee sur Test et Train est limite a Nmax.\n",
        "\n",
        "\n",
        "  #on detruit les data precedentes\n",
        "  shutil.rmtree(path_out, ignore_errors=True)\n",
        "\n",
        "  #on cree les repertoires avec la bonne arborescence\n",
        "  os.mkdir(path_out)\n",
        "  for data_type in data_types:\n",
        "    os.mkdir(os.path.join(path_out, data_type))\n",
        "    for cat in cats:\n",
        "        os.mkdir(os.path.join(path_out, data_type, cat))\n",
        "\n",
        "  #plt.figure()\n",
        "  deb = time.time()\n",
        "  cpteur = 1\n",
        "  freq_affiche = 500\n",
        "  for cat in cats:\n",
        "    n_data_available = len(os.listdir(os.path.join(path_in, cat, 'images')))\n",
        "    n_data = min(Nmax, n_data_available)\n",
        "    new_idx = np.arange(1,n_data_available)         #on prend tous les indices possibles\n",
        "    np.random.shuffle(new_idx)                      #on les melange\n",
        "    new_idx = new_idx[:n_data]                      # on ne prend que les n_data premiers elements\n",
        "    n_test = test_size * n_data // 100    \n",
        "    n_val = val_size * n_data // 100    \n",
        "    print(\"Pour la categorie {}, on va copier {} images (parmi {} disponibles) dont {} dans l'ensemble Test et {} dans l'ensemble validation\".format(cat, n_data, n_data_available, n_test, n_val))\n",
        "    for i, idx in enumerate(new_idx):\n",
        "      source = os.path.join(path_in, cat, 'images', cat)+'-'+str(idx+1)+'.png'\n",
        "      #on definit le repertoire d'arrivee\n",
        "      if i < n_test:\n",
        "        dest = os.path.join(path_out, data_types[0], cat, cat)+'-'+str(idx+1)+'.png' #on garde le meme nom d'image\n",
        "      elif i < n_val + n_test:\n",
        "        dest = os.path.join(path_out, data_types[1], cat, cat)+'-'+str(idx+1)+'.png' #on garde le meme nom d'image\n",
        "      else:\n",
        "        dest = os.path.join(path_out, data_types[2], cat, cat)+'-'+str(idx+1)+'.png' #on garde le meme nom d'image\n",
        "\n",
        "      if histo_equalize:\n",
        "        img = load_img(source, color_mode='grayscale')\n",
        "        #plt.subplot(121)\n",
        "        #plt.imshow(img, cmap='gray')\n",
        "        array = img_to_array(img, dtype='uint16').reshape(target_size)\n",
        "        new_img = exposure.equalize_adapthist(array, clip_limit=0.03)\n",
        "        #plt.subplot(122)\n",
        "        #plt.imshow(new_img, cmap='gray')\n",
        "        #plt.show()\n",
        "        save_img(dest, new_img.reshape([target_size[0], target_size[1], 1]))\n",
        "#        img = cv2.imread(source, cv2.IMREAD_GRAYSCALE)\n",
        "#        print(img.shape, img.min(), img.max())\n",
        "#        new_img = exposure.equalize_adapthist(img, clip_limit=0.03)\n",
        "#        imsave(dest, img_as_ubyte(new_img))\n",
        "      else:        \n",
        "        shutil.copy(source,dest)\n",
        "\n",
        "      if cpteur%freq_affiche == 0:\n",
        "        print(\"{} images copiees en {} secondes. {} images copiees a ce stade\".format(freq_affiche, time.time() - deb, cpteur))\n",
        "        deb = time.time()\n",
        "      cpteur += 1\n",
        " \n",
        "  if save_archive:\n",
        "    shutil.make_archive(base_name=filename, format=\"zip\", root_dir=path_out)\n"
      ],
      "metadata": {
        "id": "QNMoAti4NAwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_and_split_data(test_size, val_size, cats, data_types, Nmax, histo_equalize, path_data, path_out, save_archive, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEB4jdFpNDr2",
        "outputId": "d60690c8-93f8-410a-f0f7-9bea1640a907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pour la categorie COVID, on va copier 3616 images (parmi 3616 disponibles) dont 542 dans l'ensemble Test et 542 dans l'ensemble validation\n",
            "500 images copiees en 15.289548873901367 secondes. 500 images copiees a ce stade\n",
            "500 images copiees en 15.280012130737305 secondes. 1000 images copiees a ce stade\n",
            "500 images copiees en 15.319929838180542 secondes. 1500 images copiees a ce stade\n",
            "500 images copiees en 15.023252248764038 secondes. 2000 images copiees a ce stade\n",
            "500 images copiees en 15.595375299453735 secondes. 2500 images copiees a ce stade\n",
            "500 images copiees en 15.020164489746094 secondes. 3000 images copiees a ce stade\n",
            "500 images copiees en 15.223627805709839 secondes. 3500 images copiees a ce stade\n",
            "Pour la categorie Lung_Opacity, on va copier 6012 images (parmi 6012 disponibles) dont 901 dans l'ensemble Test et 901 dans l'ensemble validation\n",
            "500 images copiees en 15.118233680725098 secondes. 4000 images copiees a ce stade\n",
            "500 images copiees en 15.064002275466919 secondes. 4500 images copiees a ce stade\n",
            "500 images copiees en 15.18333625793457 secondes. 5000 images copiees a ce stade\n",
            "500 images copiees en 15.08078646659851 secondes. 5500 images copiees a ce stade\n",
            "500 images copiees en 15.154454946517944 secondes. 6000 images copiees a ce stade\n",
            "500 images copiees en 21.227723360061646 secondes. 6500 images copiees a ce stade\n",
            "500 images copiees en 17.48454737663269 secondes. 7000 images copiees a ce stade\n",
            "500 images copiees en 15.534020900726318 secondes. 7500 images copiees a ce stade\n",
            "500 images copiees en 15.432224988937378 secondes. 8000 images copiees a ce stade\n",
            "500 images copiees en 15.01533317565918 secondes. 8500 images copiees a ce stade\n",
            "500 images copiees en 15.258032083511353 secondes. 9000 images copiees a ce stade\n",
            "500 images copiees en 15.291797399520874 secondes. 9500 images copiees a ce stade\n",
            "Pour la categorie Normal, on va copier 10192 images (parmi 10192 disponibles) dont 1528 dans l'ensemble Test et 1528 dans l'ensemble validation\n",
            "500 images copiees en 15.453877210617065 secondes. 10000 images copiees a ce stade\n",
            "500 images copiees en 15.13999891281128 secondes. 10500 images copiees a ce stade\n",
            "500 images copiees en 14.989919900894165 secondes. 11000 images copiees a ce stade\n",
            "500 images copiees en 15.049414157867432 secondes. 11500 images copiees a ce stade\n",
            "500 images copiees en 15.241073608398438 secondes. 12000 images copiees a ce stade\n",
            "500 images copiees en 15.030908107757568 secondes. 12500 images copiees a ce stade\n",
            "500 images copiees en 14.949631214141846 secondes. 13000 images copiees a ce stade\n",
            "500 images copiees en 15.591532945632935 secondes. 13500 images copiees a ce stade\n",
            "500 images copiees en 14.89014220237732 secondes. 14000 images copiees a ce stade\n",
            "500 images copiees en 14.896482944488525 secondes. 14500 images copiees a ce stade\n",
            "500 images copiees en 14.894476890563965 secondes. 15000 images copiees a ce stade\n",
            "500 images copiees en 15.166825532913208 secondes. 15500 images copiees a ce stade\n",
            "500 images copiees en 15.088965892791748 secondes. 16000 images copiees a ce stade\n",
            "500 images copiees en 15.174758434295654 secondes. 16500 images copiees a ce stade\n",
            "500 images copiees en 15.270479202270508 secondes. 17000 images copiees a ce stade\n",
            "500 images copiees en 15.645585536956787 secondes. 17500 images copiees a ce stade\n",
            "500 images copiees en 15.70113229751587 secondes. 18000 images copiees a ce stade\n",
            "500 images copiees en 15.682886600494385 secondes. 18500 images copiees a ce stade\n",
            "500 images copiees en 15.112534761428833 secondes. 19000 images copiees a ce stade\n",
            "500 images copiees en 15.200335502624512 secondes. 19500 images copiees a ce stade\n",
            "Pour la categorie Viral Pneumonia, on va copier 1345 images (parmi 1345 disponibles) dont 201 dans l'ensemble Test et 201 dans l'ensemble validation\n",
            "500 images copiees en 15.09326457977295 secondes. 20000 images copiees a ce stade\n",
            "500 images copiees en 14.913589715957642 secondes. 20500 images copiees a ce stade\n",
            "500 images copiees en 14.839197158813477 secondes. 21000 images copiees a ce stade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tot_img = 0\n",
        "for cat in cats:\n",
        "  n_data_available = len(os.listdir(os.path.join(path_data, cat, 'images')))\n",
        "  tot_img += n_data_available\n",
        "  print('dans le repertoire de base, la categorie {} possede {} images'.format(cat, n_data_available))\n",
        "print(\"total images disponibles: \", tot_img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyftY1prL8_I",
        "outputId": "2750be5b-7c95-4605-8920-da07097e1185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dans le repertoire de base, la categorie COVID possede 3616 images\n",
            "dans le repertoire de base, la categorie Lung_Opacity possede 6012 images\n",
            "dans le repertoire de base, la categorie Normal possede 10192 images\n",
            "dans le repertoire de base, la categorie Viral Pneumonia possede 1345 images\n",
            "total images disponibles:  21165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data_type in data_types:\n",
        "  print(data_type)\n",
        "  n_type = 0\n",
        "  for cat in cats:    \n",
        "    print('la categorie {} possede {} images'.format(cat,len(os.listdir(os.path.join(path_out, data_type, cat)))))\n",
        "    n_type += len(os.listdir(os.path.join(path_out, data_type, cat)))\n",
        "  print('les donnees de {} comportent {} images'.format(data_type, n_type))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7sY47oTJ-ph",
        "outputId": "c2154f80-f06c-4121-ea0d-f9f38c212108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test\n",
            "la categorie COVID possede 542 images\n",
            "la categorie Lung_Opacity possede 901 images\n",
            "la categorie Normal possede 1528 images\n",
            "la categorie Viral Pneumonia possede 201 images\n",
            "les donnees de Test comportent 3172 images\n",
            "Validation\n",
            "la categorie COVID possede 542 images\n",
            "la categorie Lung_Opacity possede 901 images\n",
            "la categorie Normal possede 1528 images\n",
            "la categorie Viral Pneumonia possede 201 images\n",
            "les donnees de Validation comportent 3172 images\n",
            "Train\n",
            "la categorie COVID possede 2531 images\n",
            "la categorie Lung_Opacity possede 4209 images\n",
            "la categorie Normal possede 7135 images\n",
            "la categorie Viral Pneumonia possede 942 images\n",
            "les donnees de Train comportent 14817 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#verification\n",
        "os.mkdir('/content/covid19-dataset_for_ffd/')\n",
        "with ZipFile(filename+\".zip\", 'r') as zip_ref:\n",
        "  zip_ref.extractall(path='/content/covid19-dataset_for_ffd/')\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "_-YKf4jMOwM4",
        "outputId": "87ecfa3e-af8d-460d-b2d7-9ec01d4124dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-aa985a512845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/covid19-dataset_for_ffd/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/covid19-dataset_for_ffd/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/covid19-dataset_for_ffd/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EHc6FExEUX7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([1,2,3,4,5,6])\n",
        "np.random.shuffle(a)\n",
        "print(a)\n",
        "\n",
        "b = np.arange(1,100)\n",
        "print(b)\n",
        "np.random.shuffle(b)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06s1cBrLHrjs",
        "outputId": "64ae927d-15d8-41d0-9bc0-6fba18576dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 2 1 6 4 3]\n",
            "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
            " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
            " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
            " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
            " 97 98 99]\n",
            "[37 32 22 20 90 13  7 40 70 43 41 62  3 93 11 50 39  4 84 16 21 18  1 27\n",
            " 66 98 73 35 79 24 78 72 61 52 81 31 48 77 96 10 57 82 29 25 56 17  5 75\n",
            "  9 49 65 59 83 86 55 19 33 53 88 92 63 67 46 38 69 64 28 45 80 34  8 89\n",
            " 30 91 47 97 87 44 54 95 36  2 12 74 85 26 42  6 23 14 99 58 71 76 15 94\n",
            " 68 51 60]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    new_idx1 = np.arange(1,10000)         #on prend tous les indices possibles\n",
        "    np.random.shuffle(new_idx1)                      #on les melange\n",
        "    new_idx1 = new_idx1[:100]                      # on ne prend que les n_data premiers elements\n",
        "    print(new_idx1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a92QEapbKVT6",
        "outputId": "a5c5197c-9a04-484b-9b8d-0cf9f27caaf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1887 7454 7584 7529 7322 3689 4244 6449  357 6585 3559  826 3055 1357\n",
            " 6036 2453 4487  430 2189 9593 9679  457 5634 2335 2411 5278 5801 9823\n",
            " 3780 7946 2095 8773 3430 4407 5860 8187  757 4209 7395 5312  713 6551\n",
            " 9791 6912  538  465 4694 2496 4397 2978 3032 1769 2150 1503 5579   46\n",
            " 6886 1773 3378  235 7561 4841 8810 7562 8023  152 8397 4380 5565 5756\n",
            " 9965 6549 5918 4777 8298 5813 1756 9556 8297 4224 3343 4998 2000 1730\n",
            " 1359 8442 2254 8924 3907 3461 9192 4365  291 3646 9193 7767  466 3751\n",
            " 3040 3070]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#avec randint, on peut avoir 2 fois le meme indice !! ne pas utiliser\n",
        "a = np.random.randint(1,100,100)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpbtxSGLGcgn",
        "outputId": "3436aa22-45de-46c1-8e21-940877984129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[81 35 16 89 23 90 85 44 34 40 68 95 50 26 37 26 83 92 94 76 78 20 68  6\n",
            " 54 84 20  8  3 27 87 25 14 53 47  1 70 57  2 35 87 75 86 78 62 17 22 10\n",
            " 51 75 81 32 83 99 20 16 18 82 83 94 64 13 64 46 37 11 84 27 50 74 66 24\n",
            " 45 93 88 14  7 53 98 63 52  3 35 87 60 69 25 33 64 38 89 43 70 39 47 55\n",
            " 73 79 75 39]\n"
          ]
        }
      ]
    }
  ]
}